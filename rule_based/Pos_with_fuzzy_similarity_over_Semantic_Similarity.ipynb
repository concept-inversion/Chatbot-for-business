{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def select_noun_verbs_base(sentence, list_only=False):                                                                                                                                                 \n",
    "    t_sentence = nltk.word_tokenize(sentence)                                                                            \n",
    "    tags_words = nltk.pos_tag(t_sentence)\n",
    "    select_words = [(word,tag) for word,tag in tags_words if tag in ('NN','VB')]\n",
    "    if list_only:\n",
    "        return [word for word,tag in select_words]\n",
    "    return select_words\n",
    "\n",
    "def select_noun_verbs(sentence, list_only=False):                                                                                                                                                      \n",
    "    t_sentence = nltk.word_tokenize(sentence)                                                                            \n",
    "    tags_words = nltk.pos_tag(t_sentence)\n",
    "    select_words = [(word,tag) for word,tag in tags_words if tag.startswith('NN') or \n",
    "        tag.startswith('VB')]\n",
    "    if list_only:\n",
    "        return [word for word,tag in select_words]\n",
    "    return select_words\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    return lemma.lemmatize(word)\n",
    "\n",
    "def fuzzymatcher(question, query, partial=False):\n",
    "    if partial:\n",
    "        return fuzz.partial_ratio(question, query)\n",
    "    return fuzz.ratio(question, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "from chatbot.faq_db import set_all_keys\n",
    "all_questions = set_all_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_pos = [select_noun_verbs(que[0],list_only=True) for que in all_questions][:-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def words(text):\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "\n",
    "WORDS = Counter(words(open('chatbot/big.txt').read()))\n",
    "\n",
    "\n",
    "def P(word, N=sum(WORDS.values())):\n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "\n",
    "def correction(word):\n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "\n",
    "def candidates(word):\n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "\n",
    "def known(words):\n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "\n",
    "def edits2(word):\n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "\n",
    "# Synonym will be the combined list of Topic modeled words 'K1, K2, ...' and their synonyms 'S1, S2, ...'\n",
    "synonym_r = ['register', 'registration']\n",
    "synonym_m = ['merojob', 'merojobs']\n",
    "synonym_p = ['password']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python/NLTK implementation of algorithm to detect similarity between\n",
    "# short sentences described in the paper - \"Sentence Similarity based\n",
    "# on Semantic Nets and Corpus Statistics\" by Li, et al.\n",
    "# Results achieved are NOT identical to that reported in the paper, but\n",
    "# this is very likely due to the differences in the way the algorithm was\n",
    "# described in the paper and how I implemented it.\n",
    "from __future__ import division\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import brown\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Parameters to the algorithm. Currently set to values that was reported\n",
    "# in the paper to produce \"best\" results.\n",
    "ALPHA = 0.2\n",
    "BETA = 0.45\n",
    "ETA = 0.4\n",
    "PHI = 0.2\n",
    "\n",
    "brown_freqs = dict()\n",
    "N = 0\n",
    "\n",
    "query_synsets = None\n",
    "\n",
    "######################### word similarity ##########################\n",
    "\n",
    "def set_query_synsets(word_2):\n",
    "    global query_synsets\n",
    "    if not query_synsets:\n",
    "        query_synsets = wn.synsets(word_2)\n",
    "    return\n",
    "\n",
    "def get_best_synset_pair(word_1, word_2):\n",
    "    \"\"\"\n",
    "    Choose the pair with highest path similarity among all pairs.\n",
    "    Mimics pattern-seeking behavior of humans.\n",
    "    \"\"\"\n",
    "    synsets_1 = wn.synsets(word_1)\n",
    "    set_query_synsets(word_2)\n",
    "    synsets_2 = query_synsets\n",
    "    if len(synsets_1) == 0 or len(synsets_2) == 0:\n",
    "        return None, None\n",
    "    else:\n",
    "        max_sim = -1.0\n",
    "        best_pair = None, None\n",
    "        for synset_1 in synsets_1:\n",
    "            for synset_2 in synsets_2:\n",
    "                sim = wn.path_similarity(synset_1, synset_2)\n",
    "                if sim if sim is not None else -2.0 > max_sim:\n",
    "                    max_sim = sim\n",
    "                    best_pair = synset_1, synset_2\n",
    "        return best_pair\n",
    "\n",
    "\n",
    "def length_dist(synset_1, synset_2):\n",
    "    \"\"\"\n",
    "    Return a measure of the length of the shortest path in the semantic\n",
    "    ontology (Wordnet in our case as well as the paper's) between two\n",
    "    synsets.\n",
    "    \"\"\"\n",
    "    l_dist = sys.maxsize\n",
    "    if synset_1 is None or synset_2 is None:\n",
    "        return 0.0\n",
    "    if synset_1 == synset_2:\n",
    "        # if synset_1 and synset_2 are the same synset return 0\n",
    "        l_dist = 0.0\n",
    "    else:\n",
    "        wset_1 = set([str(x.name()) for x in synset_1.lemmas()])\n",
    "        wset_2 = set([str(x.name()) for x in synset_2.lemmas()])\n",
    "        if len(wset_1.intersection(wset_2)) > 0:\n",
    "            # if synset_1 != synset_2 but there is word overlap, return 1.0\n",
    "            l_dist = 1.0\n",
    "        else:\n",
    "            # just compute the shortest path between the two\n",
    "            l_dist = synset_1.shortest_path_distance(synset_2)\n",
    "            if l_dist is None:\n",
    "                l_dist = 0.0\n",
    "    # normalize path length to the range [0,1]\n",
    "    return math.exp(-ALPHA * l_dist)\n",
    "\n",
    "\n",
    "def hierarchy_dist(synset_1, synset_2):\n",
    "    \"\"\"\n",
    "    Return a measure of depth in the ontology to model the fact that\n",
    "    nodes closer to the root are broader and have less semantic similarity\n",
    "    than nodes further away from the root.\n",
    "    \"\"\"\n",
    "    h_dist = sys.maxsize\n",
    "    if synset_1 is None or synset_2 is None:\n",
    "        return h_dist\n",
    "    if synset_1 == synset_2:\n",
    "        # return the depth of one of synset_1 or synset_2\n",
    "        h_dist = max([x[1] for x in synset_1.hypernym_distances()])\n",
    "    else:\n",
    "        # find the max depth of least common subsumer\n",
    "        hypernyms_1 = {x[0]: x[1] for x in synset_1.hypernym_distances()}\n",
    "        hypernyms_2 = {x[0]: x[1] for x in synset_2.hypernym_distances()}\n",
    "        lcs_candidates = set(hypernyms_1.keys()).intersection(\n",
    "            set(hypernyms_2.keys()))\n",
    "        if len(lcs_candidates) > 0:\n",
    "            lcs_dists = []\n",
    "            for lcs_candidate in lcs_candidates:\n",
    "                lcs_d1 = 0\n",
    "                if lcs_candidate in hypernyms_1:\n",
    "                    lcs_d1 = hypernyms_1[lcs_candidate]\n",
    "                lcs_d2 = 0\n",
    "                if lcs_candidate in hypernyms_2:\n",
    "                    lcs_d2 = hypernyms_2[lcs_candidate]\n",
    "                lcs_dists.append(max([lcs_d1, lcs_d2]))\n",
    "            h_dist = max(lcs_dists)\n",
    "        else:\n",
    "            h_dist = 0\n",
    "    return ((math.exp(BETA * h_dist) - math.exp(-BETA * h_dist)) /\n",
    "            (math.exp(BETA * h_dist) + math.exp(-BETA * h_dist)))\n",
    "\n",
    "\n",
    "def word_similarity(word_1, word_2):\n",
    "    synset_pair = get_best_synset_pair(word_1, word_2)\n",
    "    return (length_dist(synset_pair[0], synset_pair[1]) *\n",
    "            hierarchy_dist(synset_pair[0], synset_pair[1]))\n",
    "\n",
    "\n",
    "######################### sentence similarity ##########################\n",
    "\n",
    "def most_similar_word(word, word_set):\n",
    "    \"\"\"\n",
    "    Find the word in the joint word set that is most similar to the word\n",
    "    passed in. We use the algorithm above to compute word similarity between\n",
    "    the word and each word in the joint word set, and return the most similar\n",
    "    word and the actual similarity value.\n",
    "    \"\"\"\n",
    "    max_sim = -1.0\n",
    "    sim_word = \"\"\n",
    "    for ref_word in word_set:\n",
    "        sim = word_similarity(word, ref_word)\n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            sim_word = ref_word\n",
    "    return sim_word, max_sim\n",
    "\n",
    "\n",
    "def info_content(lookup_word):\n",
    "    \"\"\"\n",
    "    Uses the Brown corpus available in NLTK to calculate a Laplace\n",
    "    smoothed frequency distribution of words, then uses this information\n",
    "    to compute the information content of the lookup_word.\n",
    "    \"\"\"\n",
    "    global N\n",
    "    if N == 0:\n",
    "        # poor man's lazy evaluation\n",
    "        for sent in brown.sents():\n",
    "            for word in sent:\n",
    "                word = word.lower()\n",
    "                if not word in brown_freqs:\n",
    "                    brown_freqs[word] = 0\n",
    "                brown_freqs[word] = brown_freqs[word] + 1\n",
    "                N = N + 1\n",
    "    lookup_word = lookup_word.lower()\n",
    "    n = 0 if not lookup_word in brown_freqs else brown_freqs[lookup_word]\n",
    "    return 1.0 - (math.log(n + 1) / math.log(N + 1))\n",
    "\n",
    "\n",
    "def semantic_vector(words, joint_words, info_content_norm):\n",
    "    \"\"\"\n",
    "    Computes the semantic vector of a sentence. The sentence is passed in as\n",
    "    a collection of words. The size of the semantic vector is the same as the\n",
    "    size of the joint word set. The elements are 1 if a word in the sentence\n",
    "    already exists in the joint word set, or the similarity of the word to the\n",
    "    most similar word in the joint word set if it doesn't. Both values are\n",
    "    further normalized by the word's (and similar word's) information content\n",
    "    if info_content_norm is True.\n",
    "    \"\"\"\n",
    "    sent_set = set(words)\n",
    "    semvec = np.zeros(len(joint_words))\n",
    "    i = 0\n",
    "    for joint_word in joint_words:\n",
    "        if joint_word in sent_set:\n",
    "            # if word in union exists in the sentence, s(i) = 1 (unnormalized)\n",
    "            semvec[i] = 1.0\n",
    "            if info_content_norm:\n",
    "                semvec[i] = semvec[i] * math.pow(info_content(joint_word), 2)\n",
    "        else:\n",
    "            # find the most similar word in the joint set and set the sim value\n",
    "            sim_word, max_sim = most_similar_word(joint_word, sent_set)\n",
    "            semvec[i] = PHI if max_sim > PHI else 0.0\n",
    "            if info_content_norm:\n",
    "                semvec[i] = semvec[i] * info_content(joint_word) * info_content(sim_word)\n",
    "        i = i + 1\n",
    "    return semvec\n",
    "\n",
    "\n",
    "def semantic_similarity(sentence_1, sentence_2, info_content_norm):\n",
    "    \"\"\"\n",
    "    Computes the semantic similarity between two sentences as the cosine\n",
    "    similarity between the semantic vectors computed for each sentence.\n",
    "    \"\"\"\n",
    "    words_1 = nltk.word_tokenize(sentence_1)\n",
    "    words_2 = nltk.word_tokenize(sentence_2)\n",
    "    joint_words = set(words_1).union(set(words_2))\n",
    "    vec_1 = semantic_vector(words_1, joint_words, info_content_norm)\n",
    "    vec_2 = semantic_vector(words_2, joint_words, info_content_norm)\n",
    "    return np.dot(vec_1, vec_2.T) / (np.linalg.norm(vec_1) * np.linalg.norm(vec_2))\n",
    "\n",
    "\n",
    "######################### word order similarity ##########################\n",
    "\n",
    "def word_order_vector(words, joint_words, windex):\n",
    "    \"\"\"\n",
    "    Computes the word order vector for a sentence. The sentence is passed\n",
    "    in as a collection of words. The size of the word order vector is the\n",
    "    same as the size of the joint word set. The elements of the word order\n",
    "    vector are the position mapping (from the windex dictionary) of the\n",
    "    word in the joint set if the word exists in the sentence. If the word\n",
    "    does not exist in the sentence, then the value of the element is the\n",
    "    position of the most similar word in the sentence as long as the similarity\n",
    "    is above the threshold ETA.\n",
    "    \"\"\"\n",
    "    wovec = np.zeros(len(joint_words))\n",
    "    i = 0\n",
    "    wordset = set(words)\n",
    "    for joint_word in joint_words:\n",
    "        if joint_word in wordset:\n",
    "            # word in joint_words found in sentence, just populate the index\n",
    "            wovec[i] = windex[joint_word]\n",
    "        else:\n",
    "            # word not in joint_words, find most similar word and populate\n",
    "            # word_vector with the thresholded similarity\n",
    "            sim_word, max_sim = most_similar_word(joint_word, wordset)\n",
    "            if max_sim > ETA:\n",
    "                wovec[i] = windex[sim_word]\n",
    "            else:\n",
    "                wovec[i] = 0\n",
    "        i = i + 1\n",
    "    return wovec\n",
    "\n",
    "\n",
    "def word_order_similarity(sentence_1, sentence_2):\n",
    "    \"\"\"\n",
    "    Computes the word-order similarity between two sentences as the normalized\n",
    "    difference of word order between the two sentences.\n",
    "    \"\"\"\n",
    "    words_1 = nltk.word_tokenize(sentence_1)\n",
    "    words_2 = nltk.word_tokenize(sentence_2)\n",
    "    joint_words = list(set(words_1).union(set(words_2)))\n",
    "    windex = {x[1]: x[0] for x in enumerate(joint_words)}\n",
    "    r1 = word_order_vector(words_1, joint_words, windex)\n",
    "    r2 = word_order_vector(words_2, joint_words, windex)\n",
    "    return 1.0 - (np.linalg.norm(r1 - r2) / np.linalg.norm(r1 + r2))\n",
    "\n",
    "\n",
    "######################### overall similarity ##########################\n",
    "\n",
    "def similarity(sentence_1, sentence_2, info_content_norm):\n",
    "    \"\"\"\n",
    "    Calculate the semantic similarity between two sentences. The last\n",
    "    parameter is True or False depending on whether information content\n",
    "    normalization is desired or not.\n",
    "    \"\"\"\n",
    "    t0 = time.process_time()\n",
    "#     value =  DELTA * semantic_similarity(sentence_1, sentence_2, info_content_norm) + \\\n",
    "#            (1.0 - DELTA) * word_order_similarity(sentence_1, sentence_2)\n",
    "    value =  semantic_similarity(sentence_1, sentence_2, info_content_norm)\n",
    "    t1 = time.process_time()\n",
    "    return t1-t0, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">how do i change password\n",
      "Total time :  2.7343272750000125\n",
      "[(0.9273211893902203, 'change password'), (0.6708184227959775, 'have forgotten password'), (0.10550357821675872, 'do charge service cost'), (0.09414779206024372, 'question is unanswered'), (0.06435329983497182, 'do charge job placement')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from chatbot.similarity import * # Uncomment for best result\n",
    "user_input = input('>')\n",
    "user_input_corrected = ' '.join([correction(word) for word in nltk.word_tokenize(user_input)]) \n",
    "user_input_corrected_pos_tagged = select_noun_verbs(user_input_corrected, list_only=True)\n",
    "user_input_corrected_pos_tagged_lemmatized = ' '.join(lemmatize_word(w) for w in user_input_corrected_pos_tagged)\n",
    "# que = ' '.join(user_input_corrected_pos_tagged_lemmatized)\n",
    "que = user_input_corrected_pos_tagged_lemmatized\n",
    "que_value = list()\n",
    "for each_question in questions_pos:\n",
    "    jquestion = ' '.join(each_question)\n",
    "    que_value.append((fuzzymatcher(que, jquestion, partial=False),jquestion))\n",
    "\n",
    "# print('Selected :: {}'.format(max(que_value)))\n",
    "t_list = sorted(que_value, reverse=True)[:5]\n",
    "similarity_list = list()\n",
    "now_time = time.process_time()\n",
    "for each_question in t_list:\n",
    "    _, quest = each_question\n",
    "#     print(que)\n",
    "    time_taken, temp = similarity(quest, que, True)\n",
    "    data = (temp, quest)\n",
    "    similarity_list.append(data)\n",
    "#     print(time_taken, quest, que)\n",
    "print('Total time : ', time.process_time()-now_time)\n",
    "print(sorted(similarity_list, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndo i need to pay\\n0.7019469820000026\\n1.4340822010000025\\n0.8847045580000028\\n0.988349558000003\\n1.0361885299999969\\n1.138313035000003\\n1.4684960479999987\\n1.1225306510000053\\n1.260675458999998\\n1.0737242870000046\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "do i need to pay\n",
    "0.7019469820000026\n",
    "1.4340822010000025\n",
    "0.8847045580000028\n",
    "0.988349558000003\n",
    "1.0361885299999969\n",
    "1.138313035000003\n",
    "1.4684960479999987\n",
    "1.1225306510000053\n",
    "1.260675458999998\n",
    "1.0737242870000046\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393.75"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(70*5**4*0.9)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1340000000000001"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(5*5*5+1)*0.9/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepbot",
   "language": "python",
   "name": "deepbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
