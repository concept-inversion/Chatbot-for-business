{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def select_noun_verbs_base(sentence, list_only=False):                                                                                                                                                 \n",
    "    t_sentence = nltk.word_tokenize(sentence)                                                                            \n",
    "    tags_words = nltk.pos_tag(t_sentence)\n",
    "    select_words = [(word,tag) for word,tag in tags_words if tag in ('NN','VB')]\n",
    "    if list_only:\n",
    "        return [word for word,tag in select_words]\n",
    "    return select_words\n",
    "\n",
    "def select_noun_verbs(sentence, list_only=False):                                                                                                                                                      \n",
    "    t_sentence = nltk.word_tokenize(sentence)                                                                            \n",
    "    tags_words = nltk.pos_tag(t_sentence)\n",
    "    select_words = [(word,tag) for word,tag in tags_words if tag.startswith('NN') or \n",
    "        tag.startswith('VB')]\n",
    "    if list_only:\n",
    "        return [word for word,tag in select_words]\n",
    "    return select_words\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    return lemma.lemmatize(word)\n",
    "\n",
    "def fuzzymatcher(question, query, partial=False):\n",
    "    if partial:\n",
    "        return fuzz.partial_ratio(question, query)\n",
    "    return fuzz.ratio(question, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "from chatbot.faq_db import set_all_keys\n",
    "all_questions = set_all_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_pos = [select_noun_verbs(que[0],list_only=True) for que in all_questions][:-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def words(text):\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "\n",
    "WORDS = Counter(words(open('chatbot/big.txt').read()))\n",
    "\n",
    "\n",
    "def P(word, N=sum(WORDS.values())):\n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "\n",
    "def correction(word):\n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "\n",
    "def candidates(word):\n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "\n",
    "def known(words):\n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "\n",
    "def edits2(word):\n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "\n",
    "# Synonym will be the combined list of Topic modeled words 'K1, K2, ...' and their synonyms 'S1, S2, ...'\n",
    "synonym_r = ['register', 'registration']\n",
    "synonym_m = ['merojob', 'merojobs']\n",
    "synonym_p = ['password']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">merojob\n",
      "Selected :: (70, 'merojob')\n",
      "Top 5\n",
      "(70, 'merojob')\n",
      "(61, 'is merojob')\n",
      "(60, 'meroJob')\n",
      "(50, 'emails merojob send')\n",
      "(45, 'Do need pay get job merojob')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "user_input = input('>')\n",
    "user_input_corrected = ' '.join([correction(word) for word in nltk.word_tokenize(user_input)]) \n",
    "user_input_corrected_pos_tagged = select_noun_verbs(user_input_corrected, list_only=True)\n",
    "user_input_corrected_pos_tagged_lemmatized = ' '.join(lemmatize_word(w) for w in user_input_corrected_pos_tagged)\n",
    "que = ' '.join(user_input_corrected_pos_tagged_lemmatized)\n",
    "que_value = list()\n",
    "for each_question in questions_pos:\n",
    "    que_value.append((fuzzymatcher(que, ' '.join(each_question), partial=False), ' '.join(each_question)))\n",
    "print('Selected :: {}'.format(max(que_value)))\n",
    "t_list = sorted(que_value, reverse=True)[:5]\n",
    "print('Top 5')\n",
    "print('\\n'.join([str(li) for li in t_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepbot",
   "language": "python",
   "name": "deepbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
